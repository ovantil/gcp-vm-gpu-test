{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import open_clip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from rich import print as rprint\n",
    "from sentence_transformers import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_clip.list_pretrained()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_start_time = time.time()\n",
    "model_name = \"ViT-H-14\"\n",
    "# model_name=\"ViT-B-32\"\n",
    "pretrained = \"laion2b_s32b_b79k\"\n",
    "# pretrained = \"laion400m_e32\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    model_name, pretrained\n",
    ")\n",
    "model.to(device)\n",
    "rprint(f\"Model loaded in {time.time() - load_start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images\n",
    "- base images: images of the product we're trying to generate a listing for, taken by us.\n",
    "- item images: images of eBay listings, taken from the eBay dataset, grouped into folders named by the item ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_image_folder = \"./data/base_images\"\n",
    "base_image_paths = [\n",
    "    os.path.join(base_image_folder, img_name)\n",
    "    for img_name in os.listdir(base_image_folder)\n",
    "]\n",
    "\n",
    "def load_and_preprocess_images(image_paths, batch_size=256):\n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i : i + batch_size]\n",
    "        imgs = [\n",
    "            cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "            for img_path in batch_paths\n",
    "        ]\n",
    "        imgs = [Image.fromarray(img).convert(\"RGB\") for img in imgs]\n",
    "        imgs = torch.stack([preprocess(img) for img in imgs]).to(device)\n",
    "        yield imgs\n",
    "\n",
    "\n",
    "def encode_images(image_paths):\n",
    "    print(f\"Encoding {len(image_paths)} images\")\n",
    "    if not isinstance(image_paths, list):\n",
    "        print(\"Image paths are not a list\")\n",
    "        image_paths = [\n",
    "            os.path.join(base_image_folder, img_name)\n",
    "            for img_name in os.listdir(base_image_folder)\n",
    "        ]\n",
    "\n",
    "    encoded_images = []\n",
    "    for batch_imgs in load_and_preprocess_images(image_paths):\n",
    "        with torch.no_grad():\n",
    "            batch_encoded_imgs = model.encode_image(batch_imgs)\n",
    "            encoded_images.append(batch_encoded_imgs)\n",
    "        print(f\"Encoded {len(encoded_images) * 256} images\")\n",
    "    return torch.cat(encoded_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_images = encode_images(base_image_paths)\n",
    "rprint(f\"images: {encoded_images.shape[0]}, features/image: {encoded_images.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: cross-similarity for source images\n",
    "Here, we're producing a matrix based on the provided source images, comparing the similarity of each image to every other image using cosine similarity. As expected, we've got a diagonal of 1s (comparing identical images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = util.pytorch_cos_sim(encoded_images, encoded_images)\n",
    "similarity_matrix = similarity_matrix.cpu().numpy()\n",
    "plt.imshow(similarity_matrix, cmap='Blues', interpolation=\"nearest\")\n",
    "plt.xticks(np.arange(len(base_image_paths)), [i for i in range(len(base_image_paths))])\n",
    "plt.yticks(np.arange(len(base_image_paths)), [i for i in range(len(base_image_paths))])\n",
    "plt.colorbar()\n",
    "plt.title(\"Similarity Matrix\")\n",
    "for i in range(len(base_image_paths)):\n",
    "    for j in range(len(base_image_paths)):\n",
    "        plt.text(\n",
    "            j, i, f\"{similarity_matrix[i, j]:.2f}\", ha=\"center\", va=\"center\", color=\"black\"\n",
    "        )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_image_folders_path = \"./data/item_images\"\n",
    "item_image_folders = os.listdir(item_image_folders_path)\n",
    "image_paths = []\n",
    "for folder in item_image_folders:\n",
    "    folder_path = os.path.join(item_image_folders_path, folder)\n",
    "    image_paths.extend(\n",
    "        [os.path.join(folder_path, img) for img in os.listdir(folder_path)]\n",
    "    )\n",
    "print(len(image_paths))\n",
    "print(type(image_paths))\n",
    "encoded_item_images = encode_images(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_image_paths = [\n",
    "    os.path.join(base_image_folder, img_name)\n",
    "    for img_name in os.listdir(base_image_folder)\n",
    "]\n",
    "\n",
    "encoded_base_images = encode_images(base_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_base_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_item_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = util.pytorch_cos_sim(encoded_item_images, encoded_base_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_image_paths = image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity_matrix.shape -> torch.Size([2263, 4])\n",
    "# The first dimension is the item images we've provded. They will match the order of item_image_paths (i.e. item_image_paths[0] will match similarity_matrix[0])\n",
    "# The second dimension is the base images we've provided. They will match the order of base_image_paths (i.e. base_image_paths[0] will match similarity_matrix[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the max index for each base image.\n",
    "max_indices = torch.argmax(similarity_matrix, dim=0)\n",
    "\n",
    "# same as above, but for base targets 1 through 4\n",
    "for base_target in range(len(base_image_paths)):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax[0].imshow(cv2.cvtColor(cv2.imread(item_image_paths[max_indices[base_target]]), cv2.COLOR_RGB2BGR))\n",
    "    ax[0].set_title(f\"Closest Image {item_image_paths[max_indices[base_target]].split('/')[-2:-1]}\")\n",
    "    ax[1].imshow(cv2.cvtColor(cv2.imread(base_image_paths[base_target]), cv2.COLOR_RGB2BGR))\n",
    "    ax[1].set_title(f'Base Image (score={similarity_matrix[max_indices[base_target], base_target]:.5f})')\n",
    "    plt.show()\n",
    "    print(f\"Closest Image {item_image_paths[max_indices[base_target]].split('/')[-2:-1]}\")\n",
    "    print(f'Base Image (score={similarity_matrix[max_indices[base_target], base_target]:.5f})')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
